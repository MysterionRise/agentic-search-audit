# Default configuration for Agentic Search Audit

site:
  url: "https://www.example.com"
  locale: "en-US"

  search:
    input_selectors:
      - 'input[type="search"]'
      - 'input[aria-label*="Search" i]'
      - 'input[name="q"]'
      - 'input[placeholder*="Search" i]'
    submit_strategy: "enter"  # or "clickSelector"
    submit_selector: null

  results:
    item_selectors:
      - '[data-test*="product-card"]'
      - '[data-testid*="product"]'
      - '.product-card'
      - 'a[href*="/p/"]'
    title_selectors:
      - 'h1'
      - 'h2'
      - 'h3'
      - '.product-title'
      - 'a[title]'
    url_attr: 'href'
    snippet_selectors:
      - '.product-subtitle'
      - '.description'
      - 'p'
    price_selectors:
      - '.product-price'
      - '[data-test*="price"]'
      - '.price'
    image_selectors:
      - 'img'
      - '[data-testid="product-image"]'

  modals:
    close_text_matches:
      - "accept"
      - "agree"
      - "continue"
      - "got it"
      - "close"
      - "dismiss"
    max_auto_clicks: 3
    wait_after_close_ms: 500

run:
  top_k: 10
  viewport_width: 1366
  viewport_height: 900
  network_idle_ms: 1200
  post_submit_ms: 800
  headless: true
  throttle_rps: 0.5  # requests per second
  seed: 42  # for reproducible LLM outputs

  # Lazy loading / infinite scroll settings
  max_scroll_attempts: 5   # Maximum scroll attempts to load more results
  scroll_step_px: 800      # Pixels to scroll per step
  scroll_pause_ms: 1500    # Pause between scrolls (ms) for content to load
  # load_more_selectors:   # CSS selectors for "Load More" / "Show More" buttons
  #   - 'button[class*="load-more" i]'
  #   - 'button[class*="show-more" i]'
  # load_more_text_patterns:  # Text patterns to match "Load More" buttons
  #   - "load more"
  #   - "show more"

  # Browser backend: "playwright" (default), "cdp", or "undetected"
  #
  # Backend selection guide:
  #   playwright   – Fast, good for sites with light or no bot detection.
  #                  Built-in stealth JS + playwright-stealth library.
  #   undetected   – Best for sites with aggressive bot detection (Akamai,
  #                  PerimeterX, DataDome). Uses undetected-chromedriver which
  #                  patches Chrome to bypass detection. Recommended for:
  #                  Macys, Sephora, Nordstrom, Target, Walmart, Best Buy.
  #   cdp          – Connects to an external Chrome / cloud browser instance.
  #                  Good when you need a real browser profile or Browserbase.
  browser_backend: playwright
  # cdp_endpoint: ws://localhost:9222          # required for "cdp" backend (or use browserbase)
  # browserbase_api_key: null                  # Browserbase cloud browser API key
  # browserbase_project_id: null               # Browserbase project ID

  # Proxy / IP rotation:
  #   proxy_url: http://user:pass@host:port    # Single proxy (used when strategy is "none")
  #   proxy_rotation_strategy: none            # "none", "per-site", or "per-query"
  #   proxy_list:                              # Pool of proxies for rotation
  #     - http://proxy1:8080
  #     - http://proxy2:8080

llm:
  provider: "vllm"  # "openai", "anthropic", "vllm", or "openrouter"
  model: "llava-hf/llava-v1.6-mistral-7b-hf"  # Vision model name for vLLM server
  max_tokens: 2000
  temperature: 0.2
  system_prompt: null  # uses default if null

  # vLLM-specific settings (only needed when provider is "vllm")
  base_url: "http://localhost:8000/v1"  # vLLM server endpoint
  api_key: null  # Optional API key (uses VLLM_API_KEY env var if null)

  # For OpenAI provider, use:
  # provider: "openai"
  # model: "gpt-4o-mini"
  # api_key: null  # uses OPENAI_API_KEY env var

  # For OpenRouter provider (cloud-based vision models), use:
  # provider: "openrouter"
  # model: "qwen/qwen-vl-plus"  # or "qwen/qwen-vl-max", "anthropic/claude-3.5-sonnet", etc.
  # base_url: "https://openrouter.ai/api/v1"  # default, can be omitted
  # api_key: null  # uses OPENROUTER_API_KEY env var

report:
  formats:
    - "md"
    - "html"
  out_dir: "./runs"

compliance:
  respect_robots_txt: true  # Set to false to ignore robots.txt (not recommended)
  user_agent: "AgenticSearchAudit/1.0"
  robots_timeout: 10.0  # Timeout for fetching robots.txt in seconds
